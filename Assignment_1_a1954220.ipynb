{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51730b3",
   "metadata": {},
   "source": [
    "# Assignment 1 - [Fazin Faizal]_[a1954220]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b17dcd",
   "metadata": {},
   "source": [
    "# Task 0: Data Visualization (0%, but encourage for understanding data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a18dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup: imports, reproducibility, device\n",
    "from pathlib import Path\n",
    "import random, numpy as np, pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "def set_seed(seed: int = 1954220):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(1954220)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Relative paths\n",
    "DATA_DIR = Path(\".\")\n",
    "TRAIN_IMG_DIR = DATA_DIR / \"train\"\n",
    "TEST_IMG_DIR  = DATA_DIR / \"test\"\n",
    "TRAIN_CSV = DATA_DIR / \"Train_label.csv\"\n",
    "TEST_CSV  = DATA_DIR / \"Test_label.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b481ac23",
   "metadata": {},
   "source": [
    "Import required libraries, fix random seeds for reproducibility, and select cuda if available (else CPU). Paths are relative so the notebook runs on any machine with the same folder structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20d8d8",
   "metadata": {},
   "source": [
    "# Task 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42045b40",
   "metadata": {},
   "source": [
    "### 1.1 Data Cleaning (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8c376db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned: (117, 2) (30, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read labels; CSVs have no headers\n",
    "train_df_raw = pd.read_csv(TRAIN_CSV, header=None, names=[\"filename\",\"label\"])\n",
    "test_df_raw  = pd.read_csv(TEST_CSV,  header=None, names=[\"filename\",\"label\"])\n",
    "\n",
    "# Programmatic cleaning: strip, drop '?' and missing, keep only existing images\n",
    "def clean_df(df: pd.DataFrame, img_dir: Path) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"filename\"] = df[\"filename\"].astype(str).str.strip()\n",
    "    df[\"label\"]    = df[\"label\"].astype(str).str.strip()\n",
    "    df = df[df[\"label\"].notna() & (df[\"label\"] != \"?\")]\n",
    "    df[\"exists\"] = df[\"filename\"].apply(lambda f: (img_dir / f).exists())\n",
    "    return df[df[\"exists\"]].drop(columns=[\"exists\"]).reset_index(drop=True)\n",
    "\n",
    "train_df = clean_df(train_df_raw, TRAIN_IMG_DIR)\n",
    "test_df  = clean_df(test_df_raw,  TEST_IMG_DIR)\n",
    "\n",
    "# Save cleaned CSVs (reproducibility)\n",
    "train_df.to_csv(DATA_DIR / \"Train_label_cleaned.csv\", index=False)\n",
    "test_df.to_csv(DATA_DIR / \"Test_label_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned:\", train_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab87b55",
   "metadata": {},
   "source": [
    "Read label CSVs (no headers), strip whitespace, drop invalid labels (e.g., ?), and remove rows pointing to missing image files. Save cleaned CSVs so later cells use a consistent dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829975a",
   "metadata": {},
   "source": [
    "### 1.2 Data Processing (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f0c0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify images open; drop unreadable to avoid loader crashes\n",
    "def is_readable(p: Path) -> bool:\n",
    "    try:\n",
    "        with Image.open(p) as im: im.verify()\n",
    "        with Image.open(p) as im: im.convert(\"RGB\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def drop_unreadable(df: pd.DataFrame, img_dir: Path, name: str) -> pd.DataFrame:\n",
    "    ok = [f for f in df[\"filename\"] if (img_dir / f).exists() and is_readable(img_dir / f)]\n",
    "    bad = set(df[\"filename\"]) - set(ok)\n",
    "    if bad: print(f\"[{name}] removed:\", len(bad))\n",
    "    return df[df[\"filename\"].isin(ok)].reset_index(drop=True)\n",
    "\n",
    "train_df = drop_unreadable(train_df, TRAIN_IMG_DIR, \"train\")\n",
    "test_df  = drop_unreadable(test_df,  TEST_IMG_DIR,  \"test\")\n",
    "\n",
    "# Resize to 64x64 and normalize to [0,1]\n",
    "IMG_SIZE = (64, 64)\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee0ba9",
   "metadata": {},
   "source": [
    "Verify each image can be opened (drop unreadable files) to avoid DataLoader crashes. Define a base transform to resize images to 64×64 and scale pixel values to [0,1] for stable CNN training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e815804",
   "metadata": {},
   "source": [
    "### 1.3 Data Split and Loader (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "485dbcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['ATALA', 'GOLD BANDED', 'MOURNING CLOAK', 'SLEEPY ORANGE', 'SOOTYWING'] | num_classes: 5\n"
     ]
    }
   ],
   "source": [
    "# 80/20 split; if a class has only 1 sample, keep it in TRAIN (can’t stratify)\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(42)\n",
    "\n",
    "labels = sorted(train_df[\"label\"].unique())\n",
    "class_to_idx = {c:i for i,c in enumerate(labels)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "num_classes = len(class_to_idx)\n",
    "print(\"Classes:\", labels, \"| num_classes:\", num_classes)\n",
    "\n",
    "df_idx = train_df.reset_index().rename(columns={\"index\":\"orig_idx\"})\n",
    "train_idx, val_idx = [], []\n",
    "for lbl, grp in df_idx.groupby(\"label\"):\n",
    "    idx = grp[\"orig_idx\"].to_numpy(); n = len(idx)\n",
    "    if n < 2:\n",
    "        train_idx += idx.tolist(); continue\n",
    "    n_val = max(1, int(round(0.2*n)))\n",
    "    val_sel = rng.choice(idx, size=n_val, replace=False)\n",
    "    train_sel = np.setdiff1d(idx, val_sel)\n",
    "    val_idx  += val_sel.tolist(); train_idx += train_sel.tolist()\n",
    "\n",
    "train_split_df = train_df.loc[train_idx].reset_index(drop=True)\n",
    "val_split_df   = train_df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "class ButterflyDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True); self.img_dir = img_dir; self.transform = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]; p = self.img_dir / r[\"filename\"]\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        img = self.transform(img) if self.transform else img\n",
    "        return img, class_to_idx[r[\"label\"]]\n",
    "\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86369624",
   "metadata": {},
   "source": [
    "Create an 80/20 train–val split per class; if a class has only one sample it stays in train (can’t stratify). Implement a custom Dataset that loads an image and maps its string label to a class index; build DataLoaders (with num_workers=0 for Windows/Jupyter stability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b1855",
   "metadata": {},
   "source": [
    "### 1.4 Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6034805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch OK: torch.Size([32, 3, 64, 64]) | Labels: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Augmentation only on training; val/test use base_transform\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = ButterflyDataset(train_split_df, TRAIN_IMG_DIR, transform=train_transform)\n",
    "val_ds   = ButterflyDataset(val_split_df,   TRAIN_IMG_DIR, transform=base_transform)\n",
    "test_ds  = ButterflyDataset(test_df,        TEST_IMG_DIR,  transform=base_transform)\n",
    "\n",
    "# Windows/Jupyter-safe loaders (no multiprocessing)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch OK:\", xb.shape, \"| Labels:\", yb.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf124b0",
   "metadata": {},
   "source": [
    "Add simple, label-preserving augmentations only to the training transform (horizontal flip and small rotation) to increase variety and reduce overfitting; validation/test use the non-augmented base transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e7362",
   "metadata": {},
   "source": [
    "# Task 2: Training Loop (7%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff48d52",
   "metadata": {},
   "source": [
    "### 2.1 Model Architecture (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6d02a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=8192, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Simple CNN built from base PyTorch blocks only (Conv/BN/ReLU/Pool + MLP head)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),  # 64->32\n",
    "            nn.Conv2d(32,64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),  # 32->16\n",
    "            nn.Conv2d(64,128,3, padding=1), nn.BatchNorm2d(128),nn.ReLU(), nn.MaxPool2d(2),  # 16->8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*8*8, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "model = SimpleCNN(num_classes).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ff81d",
   "metadata": {},
   "source": [
    "Define a compact CNN using only basic PyTorch blocks (Conv–BatchNorm–ReLU–Pool stacks) plus a small MLP head with Dropout; suitable capacity for 64×64 inputs while limiting overfitting.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ee414",
   "metadata": {},
   "source": [
    "### 2.2 Training Loop (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e80b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SGD_lr0.01_m0.9] Epoch 01 | train 1.4640/0.333 | val 1.5131/0.625\n",
      "[SGD_lr0.01_m0.9] Epoch 02 | train 0.8102/0.731 | val 1.3373/0.583\n",
      "[SGD_lr0.01_m0.9] Epoch 03 | train 0.7601/0.720 | val 1.2282/0.583\n",
      "[SGD_lr0.01_m0.9] Epoch 04 | train 0.4589/0.839 | val 1.1972/0.625\n",
      "[SGD_lr0.01_m0.9] Epoch 05 | train 0.4354/0.817 | val 1.0860/0.625\n",
      "[SGD_lr0.01_m0.9] Epoch 06 | train 0.6846/0.817 | val 1.2520/0.625\n",
      "[SGD_lr0.01_m0.9] Epoch 07 | train 0.3918/0.903 | val 1.3858/0.625\n",
      "[SGD_lr0.01_m0.9] Epoch 08 | train 0.3682/0.882 | val 1.1127/0.708\n",
      "[SGD_lr0.01_m0.9] Epoch 09 | train 0.3689/0.871 | val 1.4798/0.625\n",
      "[SGD_lr0.01_m0.9] Epoch 10 | train 0.2999/0.839 | val 0.7712/0.917\n",
      "[SGD_lr0.01_m0.9] Epoch 11 | train 0.3552/0.871 | val 0.7849/0.917\n",
      "[SGD_lr0.01_m0.9] Epoch 12 | train 0.1915/0.946 | val 1.1249/0.750\n",
      "[SGD_lr0.01_m0.9] Epoch 13 | train 0.1936/0.914 | val 0.8987/0.917\n",
      "[SGD_lr0.01_m0.9] Epoch 14 | train 0.1466/0.946 | val 1.0098/0.917\n",
      "[SGD_lr0.01_m0.9] Epoch 15 | train 0.1931/0.946 | val 0.9862/0.958\n",
      "[Adam_lr1e-3] Epoch 01 | train 2.1713/0.462 | val 1.3978/0.333\n",
      "[Adam_lr1e-3] Epoch 02 | train 2.8736/0.645 | val 1.2971/0.375\n",
      "[Adam_lr1e-3] Epoch 03 | train 1.2800/0.699 | val 1.2237/0.625\n",
      "[Adam_lr1e-3] Epoch 04 | train 0.9623/0.817 | val 1.8838/0.625\n",
      "[Adam_lr1e-3] Epoch 05 | train 0.7937/0.860 | val 2.1839/0.625\n",
      "[Adam_lr1e-3] Epoch 06 | train 0.3646/0.871 | val 2.7115/0.458\n",
      "[Adam_lr1e-3] Epoch 07 | train 0.4836/0.849 | val 3.0604/0.500\n",
      "[Adam_lr1e-3] Epoch 08 | train 0.2607/0.903 | val 3.0753/0.500\n",
      "[Adam_lr1e-3] Epoch 09 | train 0.2922/0.871 | val 2.9212/0.542\n",
      "[Adam_lr1e-3] Epoch 10 | train 0.2620/0.914 | val 2.5297/0.708\n",
      "[Adam_lr1e-3] Epoch 11 | train 0.2326/0.903 | val 2.4040/0.792\n",
      "[Adam_lr1e-3] Epoch 12 | train 0.1981/0.914 | val 2.4269/0.875\n",
      "[Adam_lr1e-3] Epoch 13 | train 0.1895/0.935 | val 2.4402/0.875\n",
      "[Adam_lr1e-3] Epoch 14 | train 0.2005/0.935 | val 2.4056/0.917\n",
      "[Adam_lr1e-3] Epoch 15 | train 0.1800/0.946 | val 2.4178/0.917\n",
      "[Adam_lr5e-4] Epoch 01 | train 2.1319/0.366 | val 1.4082/0.333\n",
      "[Adam_lr5e-4] Epoch 02 | train 1.6302/0.677 | val 1.1942/0.417\n",
      "[Adam_lr5e-4] Epoch 03 | train 1.0873/0.742 | val 1.0722/0.625\n",
      "[Adam_lr5e-4] Epoch 04 | train 0.7230/0.839 | val 1.2079/0.583\n",
      "[Adam_lr5e-4] Epoch 05 | train 0.5881/0.849 | val 1.3332/0.458\n",
      "[Adam_lr5e-4] Epoch 06 | train 0.3391/0.903 | val 1.4952/0.333\n",
      "[Adam_lr5e-4] Epoch 07 | train 0.2719/0.925 | val 1.6024/0.333\n",
      "[Adam_lr5e-4] Epoch 08 | train 0.2416/0.914 | val 1.6048/0.375\n",
      "[Adam_lr5e-4] Epoch 09 | train 0.2541/0.914 | val 1.4287/0.542\n",
      "[Adam_lr5e-4] Epoch 10 | train 0.2232/0.935 | val 1.1469/0.583\n",
      "[Adam_lr5e-4] Epoch 11 | train 0.1295/0.957 | val 1.0113/0.833\n",
      "[Adam_lr5e-4] Epoch 12 | train 0.1674/0.946 | val 0.9167/0.833\n",
      "[Adam_lr5e-4] Epoch 13 | train 0.1424/0.957 | val 0.8579/0.917\n",
      "[Adam_lr5e-4] Epoch 14 | train 0.1173/0.968 | val 0.8227/0.917\n",
      "[Adam_lr5e-4] Epoch 15 | train 0.1177/0.968 | val 0.8131/0.917\n",
      "[Adam_lr5e-4] Epoch 16 | train 0.1168/0.968 | val 0.8166/0.917\n",
      "[Adam_lr5e-4] Epoch 17 | train 0.1039/0.957 | val 0.8148/0.917\n",
      "[Adam_lr5e-4] Epoch 18 | train 0.1204/0.946 | val 0.8089/0.958\n",
      "[Adam_lr5e-4] Epoch 19 | train 0.0654/0.978 | val 0.8154/0.958\n",
      "[Adam_lr5e-4] Epoch 20 | train 0.1097/0.968 | val 0.8247/0.958\n",
      "Best val acc: 0.9583\n",
      "Best params: {'name': 'SGD_lr0.01_m0.9', 'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0001, 'epochs': 15}\n",
      "Saved best model → best_simplecnn.pth\n"
     ]
    }
   ],
   "source": [
    "# Train/eval utilities\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train(); total_loss=0.0; total_correct=0; n=0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()  # clear stale gradients each step\n",
    "        logits = model(xb); loss = criterion(logits, yb)\n",
    "        loss.backward(); optimizer.step()\n",
    "        b = xb.size(0); total_loss += loss.item()*b; total_correct += (logits.argmax(1)==yb).sum().item(); n += b\n",
    "    return total_loss/max(n,1), total_correct/max(n,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval(); total_loss=0.0; total_correct=0; n=0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb); loss = criterion(logits, yb)\n",
    "        b = xb.size(0); total_loss += loss.item()*b; total_correct += (logits.argmax(1)==yb).sum().item(); n += b\n",
    "    return total_loss/max(n,1), total_correct/max(n,1)\n",
    "\n",
    "# Three parameter settings → pick best on validation\n",
    "param_sets = [\n",
    "    {\"name\":\"SGD_lr0.01_m0.9\", \"optimizer\":\"SGD\",  \"lr\":0.01, \"momentum\":0.9, \"weight_decay\":1e-4, \"epochs\":15},\n",
    "    {\"name\":\"Adam_lr1e-3\",     \"optimizer\":\"Adam\", \"lr\":1e-3, \"betas\":(0.9,0.999), \"weight_decay\":1e-4, \"epochs\":15},\n",
    "    {\"name\":\"Adam_lr5e-4\",     \"optimizer\":\"Adam\", \"lr\":5e-4, \"betas\":(0.9,0.999), \"weight_decay\":5e-4, \"epochs\":20},\n",
    "]\n",
    "def make_optimizer(model, ps):\n",
    "    if ps[\"optimizer\"] == \"SGD\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=ps[\"lr\"], momentum=ps[\"momentum\"], weight_decay=ps[\"weight_decay\"])\n",
    "    else:\n",
    "        return torch.optim.Adam(model.parameters(), lr=ps[\"lr\"], betas=ps[\"betas\"], weight_decay=ps[\"weight_decay\"])\n",
    "\n",
    "history = []\n",
    "best_val_acc, best_state, best_params = -1.0, None, None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for ps in param_sets:\n",
    "    model = SimpleCNN(num_classes).to(device)\n",
    "    optimizer = make_optimizer(model, ps)\n",
    "    train_hist, val_hist = [], []\n",
    "    for epoch in range(1, ps[\"epochs\"]+1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        va_loss, va_acc = evaluate(model, val_loader, criterion)\n",
    "        train_hist.append((tr_loss, tr_acc)); val_hist.append((va_loss, va_acc))\n",
    "        print(f\"[{ps['name']}] Epoch {epoch:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}\")\n",
    "    history.append({\"params\": ps, \"train\": train_hist, \"val\": val_hist})\n",
    "    if val_hist[-1][1] > best_val_acc:\n",
    "        best_val_acc = val_hist[-1][1]\n",
    "        best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "        best_params = ps\n",
    "\n",
    "print(\"Best val acc:\", round(best_val_acc, 4))\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "BEST_MODEL_PATH = DATA_DIR / \"best_simplecnn.pth\"\n",
    "torch.save(best_state, BEST_MODEL_PATH)\n",
    "print(\"Saved best model →\", BEST_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93646438",
   "metadata": {},
   "source": [
    "Implement train_one_epoch (forward → loss → backward → optimizer step) and evaluate (no_grad forward) that return average loss and accuracy. Use Cross-Entropy for multi-class classification and optimizer.zero_grad() to prevent gradient accumulation.\n",
    "Run the same CNN with three hyperparameter sets (SGD vs Adam / learning rate / weight decay), track train/val metrics per epoch, and select the best configuration by validation accuracy. Save the best model weights for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfc726",
   "metadata": {},
   "source": [
    "### Final Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f2456f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.2794 | Test acc 0.900\n"
     ]
    }
   ],
   "source": [
    "best_model = SimpleCNN(num_classes).to(device)\n",
    "state = torch.load(BEST_MODEL_PATH, map_location=device)\n",
    "best_model.load_state_dict(state, strict=False)\n",
    "te_loss, te_acc = evaluate(best_model, test_loader, nn.CrossEntropyLoss())\n",
    "print(f\"Test loss {te_loss:.4f} | Test acc {te_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a123f",
   "metadata": {},
   "source": [
    "Reload the saved best weights and run a single evaluation on the held-out test set to report final loss and accuracy, avoiding any test-time tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
